{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c831b5b-3025-4177-bef5-25aaec89573a",
   "metadata": {},
   "source": [
    "# **EveryDream 2 - Jupyter Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ef97c-affb-4800-a9da-c89d67b2707f",
   "metadata": {},
   "source": [
    "# Fine-tuning Stable Diffusion base model with a photographer image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73894e-3b5e-4268-9f83-ed89bd4569f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bella Kotak\n",
    "\n",
    "**Bella Kotak** is an award-winning UK-based photographer with a strong, distinctive style. For this notebook tutorial, we will fine-tune a Stable Diffusion base model using her recent artwork.\n",
    "\n",
    "Go and check her instagram account now at [https://www.instagram.com/bellakotak](https://www.instagram.com/bellakotak) ...and be amazed!\n",
    "\n",
    "### Before fine-tuning the base model\n",
    "Hope you checked her portfolio, but if you haven't already, have a look at it now because you need to know how much the baseline model needs to learn to be able to create decent-looking synthetic images of her unique artistic vision. \n",
    "\n",
    "If you prompt the base Stable Diffusion 1.5 model with **\"a black and white photo of a woman wearing a floral crown and holding a bouquet of flowers in the style of Bella Kotak\"**, the base model will struggle to generate a picture that truly represents her style, or even follow the prompt. \n",
    "  \n",
    "![Bella before](https://drive.google.com/uc?export=view&id=1iUX_aMLQCulbcLMEMbta9GRsPk4VVG-i)\n",
    "\n",
    "### After fine-tuning the base model\n",
    "Thankfully by fine-tuning the base Stable Diffusion model using hundreds of captioned images of Bella Kotak, the ability of the base model to generate better-looking photographs based on her style is rather improved. Even the prompt is better followed. \n",
    "\n",
    "The following image was generated using the same prompt, seed, 614x512 resolution, and CFG value as the image above.\n",
    "\n",
    "![Bella after](https://drive.google.com/uc?export=view&id=1GgOyCNIFAkjsvkVcYc7U3SlgppLXMJPX)\n",
    "\n",
    "\n",
    "It's not a perfect image but yet the differences between the non-fine tuned model image and the fine-tuned one are very noticeable. That's the power of fine-tuning a Stable Diffusion base model with a custom image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912616b3-1e35-46f8-891f-6dcd3332d53a",
   "metadata": {},
   "source": [
    "# Step 1. Upload training images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041120c-8c65-495f-9f19-c5c7d905e6d5",
   "metadata": {},
   "source": [
    "### Download and extract the dataset \n",
    "\n",
    "\n",
    "We are going to download into our GPU instance an image dataset that has already been prepared. That is, images are organised in folders and each one is captioned in the following format:\n",
    "\n",
    "_\"[description of the image] in the style of Bella Kotak.jpg\"_\n",
    "\n",
    "Running the cell bellow will:\n",
    "1) Download a public ZIP file from Google Drive. \n",
    "\n",
    "2) Extract the zip file into the **input** subfolder.\n",
    "\n",
    "3) Delete the zip file so we are only left with image files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9f489-5a4d-4816-b66d-a84a2a00993c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Install gdown (to be able to download files from Google Drive)\n",
    "!pip install gdown\n",
    "\n",
    "# Download dataset\n",
    "google_id = \"1Ifk07HeqxHfCCOCvb5oDF-cdxfkfsuq-\"\n",
    "path_to_dataset = \"input/dataset.zip\"\n",
    "\n",
    "if not os.path.exists(path_to_dataset):\n",
    "    !gdown google_id -O path_to_dataset\n",
    "else:\n",
    "    print(f\"Already downloaded `{path_to_dataset}`\")\n",
    "\n",
    "# Unzip dataset into 'input' folder\n",
    "with zipfile.ZipFile(path_to_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall('input/dataset')\n",
    "\n",
    "# Remove zip file\n",
    "os.remove(path_to_dataset)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fcd56-0418-4be1-a5c3-38aa679b1aaf",
   "metadata": {},
   "source": [
    "# Step 2. Start Training\n",
    "\n",
    "Once our training images are inside the input **folder** we are ready to train the model. \n",
    "\n",
    "\n",
    "### Training configuration\n",
    "We will train with the following configuration settings:\n",
    "\n",
    "* **project name**: \"sd1_kotak\" <= Name of the project. It is convenient to name it in a way that identifies it from other training sessions.\n",
    "* **data_root**: \"input\" <= Folder location of the training images\n",
    "* **max epochs**: 100 <= An epoch refers to the one entire passing of training images through the trainer. We are doing 100 entire passes.  \n",
    "* **batch size**: 6 <= Determines the amount of images that are going to be trained every epoch\n",
    "* **sample steps**: 80 <= Determines how frequently samples are generated. In this case we will save every 20 epoch steps.   \n",
    "* **save every n epochs**: 20 <= Checkpoints will be saved every 20 epochs (since we are doing 100 epochs, we will end with 5 checkpoints) \n",
    "* **learning rate** (lr): 1.2e-6 <= The learning rate determines the pace at which our model learns. This is the default value and generally, it works for most cases. \n",
    "* **learning rate scheduler** (lr scheduler): constant <= The scheduler controls the way the learning rate is adjusted over time \n",
    "* **save ckpt dir**: \"ouput\" <= Folder location of the saved checkpoints\n",
    "\n",
    "For more information about how to configure the trainer see the related chapter or check EveryDream 2 trainer's [official documentation](https://github.com/victorchall/EveryDream2trainer/blob/main/doc/TRAINING.md). \n",
    "\n",
    "\n",
    "### Running the training session\n",
    "\n",
    "To start the training run the cell bellow. This could take a while depending on your number of images, batch size and max_epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73fb86-ebef-41e2-9382-4aa11be84be6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the training\n",
    "\n",
    "%run train.py --resume_ckpt \"learn2train/stable-diffusion-v1-5\" \\\n",
    "--project_name \"sd1_kotak\" \\\n",
    "--data_root \"input\" \\\n",
    "--max_epochs 100 \\\n",
    "--sample_steps 80 \\\n",
    "--batch_size 6 \\\n",
    "--save_every_n_epochs 20 \\\n",
    "--lr 1.2e-6 \\\n",
    "--lr_scheduler constant \\\n",
    "--save_ckpt_dir \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0982c-6029-4fc1-b161-f71c18680261",
   "metadata": {},
   "source": [
    "# Step 3. Hugging Face upload \n",
    "\n",
    "Instead of manually downlading your checkpoints into your computer, you will upload them to your very own Hugging Face repository. This offers you some advantages: First of all, it's free storage! Up to 100GB last time we checked. You can also easily share your models or showcase your work to others.  \n",
    "\n",
    "\n",
    "### Enable uploading \n",
    "\n",
    "To enable uploading to Hugging Face go to your [Hugging Face's Access Tokens page](https://huggingface.co/settings/tokens) and click the \"New Token\" button below the \"User Access Tokens\" box. \n",
    "\n",
    "When the \"Create a new access token\" window pops-up fill-in the following:\n",
    "\n",
    "* Name: **Everydream2** (or whatever you want)\n",
    "* Role: **Write**\n",
    "\n",
    "And click \"Generate a token\" to create your access token. \n",
    "\n",
    "\n",
    "### Create repository\n",
    "\n",
    "Crate your own Hugging Face repository to host your checkpoints.  Go here [Create New HF Model](https://huggingface.co/new)  \n",
    "\n",
    "* Model name: **kotak**\n",
    "* Licence: **leave it blank**\n",
    "* Private or Public: **your choice**\n",
    "\n",
    "Make sure you write down the repo name you make for future use.  You can reuse it later.\n",
    "\n",
    "\n",
    "### Log-in into your Hugging Face account\n",
    "\n",
    "Run the cell below and paste your Hugging Face token into the prompt to log into your account to be able to upload data into your repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b4a52-97ab-4d70-8db7-43d1203135fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, hf_hub_download\n",
    "import os\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24eee3d-f5df-45f3-9acc-ee0206cfe6b1",
   "metadata": {},
   "source": [
    "### Upload checkpoints to your Hugging Face account\n",
    "\n",
    "Make sure you are **logged in** using the above login cell first.\n",
    "\n",
    "Use the cell below to upload one or more checkpoints to your personal Hugging Face repository. You should already be authorized to Huggingface by token if you used the download/token cells above.\n",
    "\n",
    "When you run the cell below, a box will show up and you need to  **CLICK** to select which .ckpt files are marked for upload. This allows you to select which ones to upload.  If you don't click of the ckpts, nothing will happen.\n",
    "\n",
    "You will also be required to fill-in your username and your repository name:\n",
    "* Hugging Face username: Look for your username in [HuggingFace account page](https://huggingface.co/settings/account).\n",
    "* Hugging Face repository name: **kotak**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df5e1a-3c68-41c0-a4ed-ea0abcd19858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after reading the instructions of the cell above. \n",
    "\n",
    "import glob\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from ipywidgets import *\n",
    "\n",
    "all_ckpts = [f for f in glob.glob(\"*.ckpt\")]\n",
    "  \n",
    "ckpt_picker = SelectMultiple(options=all_ckpts, layout=Layout(width=\"600px\")) \n",
    "hfuser = Text(placeholder='Hugging Face username')\n",
    "hfrepo = Text(placeholder='Hugging Face repository name')\n",
    "\n",
    "api = HfApi()\n",
    "upload_btn = Button(description='Upload')\n",
    "out = Output()\n",
    "\n",
    "def upload_ckpts(_):\n",
    "    repo_id=f\"{hfuser.value or hfuser.placeholder}/{hfrepo.value or hfrepo.placeholder}\"\n",
    "    with out:\n",
    "        if ckpt_picker is None or len(ckpt_picker.value) < 1:\n",
    "            print(\"Nothing selected for upload, make sure to click one of the ckpt files in the list, or, you have no ckpt files in the current directory.\")\n",
    "        for ckpt in ckpt_picker.value:\n",
    "            print(f\"Uploading to HF: huggingface.co/{repo_id}/{ckpt}\")\n",
    "            response = api.upload_file(\n",
    "                path_or_fileobj=ckpt,\n",
    "                path_in_repo=ckpt,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=None,\n",
    "                create_pr=1,\n",
    "            )\n",
    "            display(response)\n",
    "        print(\"DONE\")\n",
    "\n",
    "upload_btn.on_click(upload_ckpts)\n",
    "box = VBox([ckpt_picker, HBox([hfuser, hfrepo]), upload_btn, out])\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4a28a-2488-4216-b431-fcc3e9d1de9e",
   "metadata": {},
   "source": [
    "### Accept the PRs the cell above created\n",
    "\n",
    "Go back to your Hugging Face repository and accept the PRs the above cell created to see your files to validate the uploads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a00d16-9b84-492f-8e6a-defe71e82b43",
   "metadata": {},
   "source": [
    "# Step 4. Test inference on your checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb1a8cd-6a04-44e5-a770-c23ee247ce82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28672/644195404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, DDIMScheduler, DDPMScheduler, PNDMScheduler, EulerAncestralDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "\n",
    "checkpoints_ts = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if os.path.basename(file) == \"model_index.json\":\n",
    "                ts = os.path.getmtime(os.path.join(root,file))\n",
    "                ckpt = root\n",
    "                checkpoints_ts.append((ts, root))\n",
    "\n",
    "checkpoints = [ckpt for (_, ckpt) in sorted(checkpoints_ts, reverse=True)]\n",
    "full_width = Layout(width='600px')\n",
    "half_width = Layout(width='300px')\n",
    "\n",
    "checkpoint = Dropdown(options=checkpoints, description='Checkpoint:', layout=full_width)\n",
    "prompt = Textarea(value='a photo of ', description='Prompt:', layout=full_width)\n",
    "height = IntSlider(value=512, min=256, max=768, step=32, description='Height:', layout=half_width)\n",
    "width = IntSlider(value=512, min=256, max=768, step=32, description='Width:', layout=half_width)\n",
    "cfg = FloatSlider(value=7.0, min=0.0, max=14.0, step=0.2, description='CFG Scale:', layout=half_width)\n",
    "steps = IntSlider(value=30, min=10, max=100, description='Steps:', layout=half_width)\n",
    "seed = IntText(value=-1, description='Seed:', layout=half_width)\n",
    "generate_btn = Button(description='Generate', layout=full_width)\n",
    "out = Output()\n",
    "\n",
    "def generate(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        display(f\"Loading model {checkpoint.value}\")\n",
    "        actual_seed = seed.value if seed.value != -1 else random.randint(0, 2**30)\n",
    "\n",
    "        text_encoder = CLIPTextModel.from_pretrained(checkpoint.value, subfolder=\"text_encoder\")\n",
    "        vae = AutoencoderKL.from_pretrained(checkpoint.value, subfolder=\"vae\")\n",
    "        unet = UNet2DConditionModel.from_pretrained(checkpoint.value, subfolder=\"unet\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(checkpoint.value, subfolder=\"tokenizer\", use_fast=False)\n",
    "        scheduler = DDIMScheduler.from_pretrained(checkpoint.value, subfolder=\"scheduler\")\n",
    "        text_encoder.eval()\n",
    "        vae.eval()\n",
    "        unet.eval()\n",
    "\n",
    "        text_encoder.to(\"cuda\")\n",
    "        vae.to(\"cuda\")\n",
    "        unet.to(\"cuda\")\n",
    "\n",
    "        pipe = StableDiffusionPipeline(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=None, # save vram\n",
    "            requires_safety_checker=None, # avoid nag\n",
    "            feature_extractor=None, # must be none of no safety checker\n",
    "        )\n",
    "\n",
    "        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "        \n",
    "        print(inspect.cleandoc(f\"\"\"\n",
    "              Prompt: {prompt.value}\n",
    "              Resolution: {width.value}x{height.value}\n",
    "              CFG: {cfg.value}\n",
    "              Steps: {steps.value}\n",
    "              Seed: {actual_seed}\n",
    "              \"\"\"))\n",
    "        with autocast(\"cuda\"):\n",
    "            image = pipe(prompt.value, \n",
    "                generator=torch.Generator(\"cuda\").manual_seed(actual_seed),\n",
    "                num_inference_steps=steps.value, \n",
    "                guidance_scale=cfg.value,\n",
    "                width=width.value,\n",
    "                height=height.value\n",
    "            ).images[0]\n",
    "        del pipe\n",
    "        gc.collect()\n",
    "        with torch.cuda.device(\"cuda\"):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        display(image)\n",
    "            \n",
    "generate_btn.on_click(generate)\n",
    "box = VBox(\n",
    "    children=[\n",
    "        checkpoint, prompt, \n",
    "        HBox([VBox([width, height]), VBox([steps, cfg])]), \n",
    "        seed, \n",
    "        generate_btn, \n",
    "        out]\n",
    ")\n",
    "\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca315acc-3af5-402e-8e2f-ee14d5a0c33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e677f113ff5b533036843965d6e18980b635d0aedc1c5cebd058006c5afc92a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
