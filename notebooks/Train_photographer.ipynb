{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c831b5b-3025-4177-bef5-25aaec89573a",
   "metadata": {},
   "source": [
    "# **Running, monitoring and evaluating a training job**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9a96d-668e-40e2-bb63-a24b9213948d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### About this interactive guide\n",
    "\n",
    "This Jupyter Notebook is part of our [A Step-by-Step Guide for Non-Technical Folks on Training Stable Diffusion with a low-cost Cloud GPU](https://learn2train.medium.com/a-step-by-step-guide-for-non-technical-folks-on-training-stable-diffusion-with-a-low-cost-cloud-gpu-344c6b250d64). \n",
    "\n",
    "In this guide, we'll cover the following topics in an interactive way:\n",
    "\n",
    "1. **Fine-tuning a Stable Diffusion base model with a custom dataset**.\n",
    "      \n",
    "2. **Download the training dataset**. \n",
    "\n",
    "3. **Start the training job**\n",
    "    \n",
    "4. **Monitor your sample generations in Weights & Biases (W&B)**. W&B is a free tool used to visualise machine learning experiments. No installation is required as it will be run from a standalone webpage.\n",
    "\n",
    "5. **Training is done** \n",
    "\n",
    "6. **Upload the fine-tuned models to Hugging Face (optional)** so you can re-use them later. Hugging Face is an open-source community for AI experts and enthusiasts. Itâ€™s free to use!\n",
    "       \n",
    "7. **Evaluate the fine-tuned checkpoints** to asses its performance.\n",
    "\n",
    "8. **Terminate the GPU instance**. Avoid incurring charges by destroying the GPU instance. \n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "This interactive tutorial assumes you have:\n",
    "\n",
    "- Setup the training application on a cloud GPU platform according to [this guide](https://learn2train.medium.com/a-step-by-step-guide-for-non-technical-folks-on-training-stable-diffusion-with-a-low-cost-cloud-gpu-344c6b250d64)\n",
    "- A basic understanding of how Jupyter Notebooks work (if you don't check this [cool introduction to Jupyter Notebook demo](https://jupyter.org/try-jupyter/notebooks/?path=notebooks/Intro.ipynb)!)\n",
    "- A reliable internet connection.\n",
    "- An updated browser such as Chrome, Safari, Firefox, etc. \n",
    "- Time to train (it will take about 20 minutes to train the training dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb35beb-c70a-49f7-a369-ceecdc13fd9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Fine-tuning a Stable Diffusion base model with custom data\n",
    "\n",
    "### Fine-tuning using a photographer's image dataset\n",
    "\n",
    "In this notebook tutorial we will train a Stable Diffusion 1.5 base model in the style of a photographer that it doesn't knows very well. By feeding the model with a photographer's image dataset, it should be able to generate pictures in the style of the photographer.  \n",
    "\n",
    "### Bella Kotak\n",
    "\n",
    "In this notebook tutorial, we will fine-tune a Stable Diffusion text-to-image model using Bella Kotak recent artwork. **Bella Kotak** is an award-winning UK-based photographer with a strong, distinctive style.\n",
    "\n",
    "Check her instagram account at [https://www.instagram.com/bellakotak](https://www.instagram.com/bellakotak) ...and be amazed!\n",
    "\n",
    "### Before fine-tuning the base model\n",
    "\n",
    "Hope you checked her portfolio because you need to know how much the base model needs to learn in order to be able to generate decent-looking synthetic images in her artistic style. \n",
    "\n",
    "Since the base model wasn't trained with enough pictures of her artwork, it fails to portray her unique artistic vision. So if we prompt the base Stable Diffusion 1.5 model with **\"a black and white photo of a woman wearing a floral crown and holding a bouquet of flowers in the style of Bella Kotak\"**, the base model will struggle to generate a picture that represents her style, or even follow the prompt. \n",
    "  \n",
    "![Bella before](https://drive.google.com/uc?export=view&id=1iUX_aMLQCulbcLMEMbta9GRsPk4VVG-i)\n",
    "\n",
    "### After fine-tuning the base model\n",
    "\n",
    "Thankfully by fine-tuning the base Stable Diffusion model using captioned images, the ability of the base model to generate better-looking pictures based on her style is greatly improved. And even the prompt is better followed. \n",
    "\n",
    "Image below was generated on a fine-tuned Stable Diffusion 1.5 model. It has the same prompt, seed, resolution, and CFG values as the image above!\n",
    "\n",
    "![Bella after](https://drive.google.com/uc?export=view&id=1GgOyCNIFAkjsvkVcYc7U3SlgppLXMJPX)\n",
    "\n",
    "As you can see, it's not perfect -for one thing, it's not exactly black and white- but yet the differences between the non-fine tuned model and the fine-tuned one are rather noticeable. That's the power of training a Stable Diffusion base model with a custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef18e8-b5aa-41fd-8cc8-3ad07d19307f",
   "metadata": {},
   "source": [
    "# 2. Download the training dataset\n",
    "\n",
    "### Download and extract the dataset \n",
    "\n",
    "We are going to download an already prepared training dataset into our GPU instance.\n",
    "\n",
    "A dataset is said to be prepared when every image has a caption describing it. It may or may not include other configuration settings read by the training application. \n",
    "\n",
    "Our image dataset contains 109 images, 109 text files, and 1 tag configuration file (`global.yaml`) that adds a suffix tag to each text file (in this case appends the phrase `in the style of Bella Kotak` to each caption description for each image). For more information about how to create a dataset please refer to chapter II of the tutorial.\n",
    "\n",
    "This is an example of how images and caption files are formatted in our dataset:\n",
    "\n",
    "* `image-name_001.jpg`\n",
    "* `image-name_001.txt`  <= Same filename as the jpg file\n",
    "\n",
    "The text file `image-name_001.txt` contains the caption describing `image-name_001.jpg`, say, for example: `a photo of a woman wearing a floral crown and holding a bouquet of flowers in the style of Bella Kotak`.\n",
    "\n",
    "\n",
    "\n",
    "Running the cell below will download a public ZIP file from Google Drive, extract it and store it in the **input** subfolder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9f489-5a4d-4816-b66d-a84a2a00993c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Install gdown (to be able to download files from Google Drive)\n",
    "!pip install gdown\n",
    "\n",
    "# Download dataset\n",
    "os.makedirs('input', exist_ok=True)\n",
    "path_to_dataset = \"input/dataset.zip\"\n",
    "\n",
    "if not os.path.exists(path_to_dataset):\n",
    "    !gdown 1Ifk07HeqxHfCCOCvb5oDF-cdxfkfsuq- -O input/dataset.zip\n",
    "else:\n",
    "    print(f\"Already downloaded `{path_to_dataset}`\")\n",
    "\n",
    "# Unzip dataset into 'input' folder\n",
    "with zipfile.ZipFile(path_to_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall('input/dataset')\n",
    "\n",
    "# Remove zip file\n",
    "os.remove(path_to_dataset)\n",
    "\n",
    "# List input directory\n",
    "%ls input/\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fcd56-0418-4be1-a5c3-38aa679b1aaf",
   "metadata": {},
   "source": [
    "# 3. Start the training job\n",
    "\n",
    "Once our training images and their captions are inside the **input folder** we are ready to train the model. \n",
    "\n",
    "\n",
    "### Training configuration\n",
    "We will override the following default training settings:\n",
    "\n",
    "* **project name**: \"sd1_kotak\" <= Name of the project. It is convenient to name it in a way that identifies it from other training sessions.\n",
    "* **data_root**: \"input\" <= Folder location of the training images\n",
    "* **max epochs**: 60 <= An epoch refers to the one entire passing of training images through the trainer. We are doing 60 entire passes.  \n",
    "* **batch size**: 6 <= Determines the amount of images that are going to be trained every epoch\n",
    "* **sample steps**: 80 <= Determines how frequently samples are generated. In this case we will save every 20 epoch steps.   \n",
    "* **save every n epochs**: 20 <= Checkpoints will be saved every 20 epochs (since we are doing 60 epochs, we will end with 3 checkpoints) \n",
    "* **save ckpt dir**: \"ouput\" <= Folder location of the saved checkpoints\n",
    "* **zero_frequency_noise_ratio**: 0.04 <= This will make dark scenes more realistic  \n",
    "* **optimizer_config**: optimizer-photo.json <= We add an optimiser config file to get better results\n",
    "* **cond_dropout**: 0.0 <= This will prevent the trainer learning images without captions\n",
    "\n",
    "\n",
    "The are more configurations not show here. For a detailed explanation of each check [EveryDream 2 documentation](https://github.com/victorchall/EveryDream2trainer/blob/main/doc/TRAINING.md). \n",
    "\n",
    "### Download the optimizer configuration file\n",
    "\n",
    "Run the following cell to get the optimiser configuration settings to improve our training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655d4fc-7975-4ca3-bb58-767de9177a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/learn2train/l2t-sd/main/notebooks/optimizer-photo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0141a5-f13f-46f6-a9a5-5bcf6190c0fc",
   "metadata": {},
   "source": [
    "### Set up Weights & Biases (W&B) for monitoring sample generation \n",
    "\n",
    "If you have a W&B account you can use it to track your training progress.  If you don't have one, you can create your W&B account for free at https://wandb.ai/site.\n",
    "\n",
    "You can get your API key from your [User Settings](https://wandb.ai/settings). Paste it in the following cell where it says \"PUT-YOUR-API-KEY-HERE\", keep the double quotes, and then RUN the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f74cc3-d90f-465e-9aaa-0468b83df1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb_token = \"PUT-YOUR-API-KEY-HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54fbcf-b95e-415c-ad8a-78494ff728bb",
   "metadata": {},
   "source": [
    "The cell above should look like:\n",
    "    \n",
    "`wandb_token = \"28d37291d39f337237291d39f391d39f3\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0a39e-8204-4017-98ef-4ae2451511be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running the training session\n",
    "\n",
    "To start training run the cell below. The cell will start printing its log. Keep scrolling down to monitor the current status of the training session. \n",
    "\n",
    "**IMPORTANT: If you see messages with a red backround, IGNORE THEM as they are only warning messages** \n",
    "\n",
    "The training takes about 20 minutes on a RTX 3090 with 24GB of VRAM. \n",
    "\n",
    "While you wait for the `Training completed` message, watch the samples being generated in Weights & Biases (see cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73fb86-ebef-41e2-9382-4aa11be84be6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the wandb token\n",
    "wandb_settings = \"\"\n",
    "if wandb_token:\n",
    "  !rm /root/.netrc\n",
    "  !wandb login $wandb_token\n",
    "  wandb_settings = \"--wandb\"\n",
    "\n",
    "# Start the training\n",
    "\n",
    "%run train.py --resume_ckpt \"learn2train/stable-diffusion-v1-5\" \\\n",
    "$wandb_settings \\\n",
    "--project_name \"sd1_kotak\" \\\n",
    "--data_root \"input\" \\\n",
    "--max_epochs 60 \\\n",
    "--sample_steps 80 \\\n",
    "--batch_size 6 \\\n",
    "--save_every_n_epochs 20 \\\n",
    "--zero_frequency_noise_ratio 0.04 \\\n",
    "--cond_dropout 0.0 \\\n",
    "--optimizer_config optimizer-photo.json \\\n",
    "--save_ckpt_dir \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0be597-abb7-4f52-b29e-11f18c387edb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Watch your samples in Weights & Biases while training is running\n",
    "\n",
    "### W&B dashboard\n",
    "\n",
    "Go to the [W&B dashboard](https://wandb.ai/home) in another browser tab. You will see your training run in your home page. \n",
    "\n",
    "Click on your training run to check the samples being generated. They should give you an idea how good/bad your model learning progress is going. You should stop the training once you are satisfied with the results you are seeing.\n",
    "\n",
    "Samples come in three. That is because each sample generated uses different CFG values (1, 4 and 7).\n",
    "\n",
    "![W&B](https://drive.google.com/uc?export=view&id=1G1fmv5uFN_pk57jBhmD7SVes4at-uv4C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd913f-3c0d-4014-b6e1-6b2fd9cfc2b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Training is finished\n",
    "\n",
    "Once the training is done you should see the following messages:\n",
    "\n",
    "![Training is finished](https://drive.google.com/uc?export=view&id=1WXwNcHaKStpuusvReueriEJXsl3rLWRM)\n",
    "\n",
    "\n",
    "That was it! The base model has been updated and you are now left with checkpoints.\n",
    "\n",
    "Before terminating the GPU instance, you could download the checkpoints into your computer, or upload them to your Hugging Face repository.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0982c-6029-4fc1-b161-f71c18680261",
   "metadata": {},
   "source": [
    "# 6. Upload your checkpoints to Hugging Face (optional)\n",
    "\n",
    "If you aren't downloading your checkpoints to your computer, you could save them to your Hugging Face repository.\n",
    "\n",
    "### Get a Hugging Face token\n",
    "\n",
    "If you haven't got one yet, have a look at [How to Host Stable Diffusion Checkpoints on Hugging Face for Free](https://learn2train.medium.com/a-step-by-step-guide-to-host-stable-diffusion-checkpoints-on-hugging-face-for-free-2098d0c18a01)\n",
    "\n",
    "### Log-in into your account \n",
    "\n",
    "Run the cell below and paste your **Hugging Face write token** into the prompt to log into your account to be able to upload data into your repo (NOTE: There's no need to go to the Hugging Face website: you will be loging in from the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b4a52-97ab-4d70-8db7-43d1203135fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n",
    "\n",
    "from huggingface_hub import notebook_login, hf_hub_download\n",
    "import os\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24eee3d-f5df-45f3-9acc-ee0206cfe6b1",
   "metadata": {},
   "source": [
    "### Upload checkpoints to your model repository\n",
    "\n",
    "Make sure you are **logged in** to Hugging Face running the above login cell first.\n",
    "\n",
    "Use the cell below to upload one or more checkpoints to your personal Hugging Face repository. You should already be authorized to interact with Hugging Face if you ran the cell above.\n",
    "\n",
    "When you run the cell below, a box will show up and you need to  **CLICK** to select which `.safetensor` files are marked for upload. This allows you to select which ones to upload.  If you don't click of the ckpts, nothing will happen.\n",
    "\n",
    "You will also be required to fill-in your username and your repository name:\n",
    "* Hugging Face username: **Your username** (look in [HuggingFace account page](https://huggingface.co/settings/account)).\n",
    "* Hugging Face repository name: **your-repo-name**\n",
    "\n",
    "**WARNING**\n",
    "\n",
    "**If your Hugging Face account is brand new upload only 3 checkpoint files**. For safety reasons, Hugging Face limits the amount of files a new user can make. If you try to upload more than 3 checkpoint files you'll probably get a warning tell you to wait 24 hours to keep uploading. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df5e1a-3c68-41c0-a4ed-ea0abcd19858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after reading the instructions of the cell above. \n",
    "\n",
    "import glob\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from ipywidgets import *\n",
    "\n",
    "all_ckpts = [f for f in glob.glob(\"output/*.safetensor\")]\n",
    "  \n",
    "ckpt_picker = SelectMultiple(options=all_ckpts, layout=Layout(width=\"600px\")) \n",
    "hfuser = Text(placeholder='Hugging Face username')\n",
    "hfrepo = Text(placeholder='Hugging Face repository name')\n",
    "\n",
    "api = HfApi()\n",
    "upload_btn = Button(description='Upload')\n",
    "out = Output()\n",
    "\n",
    "def upload_ckpts(_):\n",
    "    repo_id=f\"{hfuser.value or hfuser.placeholder}/{hfrepo.value or hfrepo.placeholder}\"\n",
    "    with out:\n",
    "        if ckpt_picker is None or len(ckpt_picker.value) < 1:\n",
    "            print(\"Nothing selected for upload, make sure to click one of the ckpt files in the list, or, you have no ckpt files in the current directory.\")\n",
    "        for ckpt in ckpt_picker.value:\n",
    "            print(f\"Uploading to HF: huggingface.co/{repo_id}/{ckpt}\")\n",
    "            response = api.upload_file(\n",
    "                path_or_fileobj=ckpt,\n",
    "                path_in_repo=ckpt,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=None,\n",
    "                create_pr=1,\n",
    "            )\n",
    "            display(response)\n",
    "        print(\"DONE\")\n",
    "\n",
    "upload_btn.on_click(upload_ckpts)\n",
    "box = VBox([ckpt_picker, HBox([hfuser, hfrepo]), upload_btn, out])\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4a28a-2488-4216-b431-fcc3e9d1de9e",
   "metadata": {},
   "source": [
    "### Save the uploads to your model repository\n",
    "\n",
    "To actually save the uploaded checkpoints to your repo, go back to your Hugging Face model repository and click the **Community** tab. You'll see a list of one or more checkpoints. Go one by one and click **Merge** to save them to your model repository:\n",
    "\n",
    "![Merge](https://drive.google.com/uc?export=view&id=1zyOcOq9uABW1dO69pNYenvsag1C7asyc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a00d16-9b84-492f-8e6a-defe71e82b43",
   "metadata": {},
   "source": [
    "# 7. Evaluate the fine-tuned checkpoints\n",
    "\n",
    "\n",
    "### Test inference on your checkpoints\n",
    "\n",
    "To recap: Training is over and you are left with model checkpoints (safetensor files). These checkpoints are updated fine-tuned models saved at different times during the training session. \n",
    "\n",
    "The main idea here is to evaluate each of your checkpoints to find the ones that generate the output you like the most.  \n",
    "\n",
    "Run the following cell to display a mini text-to-image generator. You can choose any checkpoint -or all of them- and set inference parameters such as **prompt, steps, CFG, resolution and seed**.\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1a8cd-6a04-44e5-a770-c23ee247ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, DDIMScheduler, DDPMScheduler, PNDMScheduler, EulerAncestralDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "\n",
    "checkpoints_ts = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if os.path.basename(file) == \"model_index.json\":\n",
    "                ts = os.path.getmtime(os.path.join(root,file))\n",
    "                ckpt = root\n",
    "                checkpoints_ts.append((ts, root))\n",
    "\n",
    "checkpoints = [ckpt for (_, ckpt) in sorted(checkpoints_ts, reverse=True)]\n",
    "full_width = Layout(width='600px')\n",
    "half_width = Layout(width='300px')\n",
    "\n",
    "checkpoint = Dropdown(options=checkpoints, description='Checkpoint:', layout=full_width)\n",
    "prompt = Textarea(value='a photo of ', description='Prompt:', layout=full_width)\n",
    "height = IntSlider(value=512, min=256, max=768, step=32, description='Height:', layout=half_width)\n",
    "width = IntSlider(value=512, min=256, max=768, step=32, description='Width:', layout=half_width)\n",
    "cfg = FloatSlider(value=7.0, min=0.0, max=14.0, step=0.2, description='CFG Scale:', layout=half_width)\n",
    "steps = IntSlider(value=30, min=10, max=100, description='Steps:', layout=half_width)\n",
    "seed = IntText(value=-1, description='Seed:', layout=half_width)\n",
    "generate_btn = Button(description='Generate', layout=full_width)\n",
    "out = Output()\n",
    "\n",
    "def generate(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        display(f\"Loading model {checkpoint.value}\")\n",
    "        actual_seed = seed.value if seed.value != -1 else random.randint(0, 2**30)\n",
    "\n",
    "        text_encoder = CLIPTextModel.from_pretrained(checkpoint.value, subfolder=\"text_encoder\")\n",
    "        vae = AutoencoderKL.from_pretrained(checkpoint.value, subfolder=\"vae\")\n",
    "        unet = UNet2DConditionModel.from_pretrained(checkpoint.value, subfolder=\"unet\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(checkpoint.value, subfolder=\"tokenizer\", use_fast=False)\n",
    "        scheduler = DDIMScheduler.from_pretrained(checkpoint.value, subfolder=\"scheduler\")\n",
    "        text_encoder.eval()\n",
    "        vae.eval()\n",
    "        unet.eval()\n",
    "\n",
    "        text_encoder.to(\"cuda\")\n",
    "        vae.to(\"cuda\")\n",
    "        unet.to(\"cuda\")\n",
    "\n",
    "        pipe = StableDiffusionPipeline(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=None, # save vram\n",
    "            requires_safety_checker=None, # avoid nag\n",
    "            feature_extractor=None, # must be none of no safety checker\n",
    "        )\n",
    "\n",
    "        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "        \n",
    "        print(inspect.cleandoc(f\"\"\"\n",
    "              Prompt: {prompt.value}\n",
    "              Resolution: {width.value}x{height.value}\n",
    "              CFG: {cfg.value}\n",
    "              Steps: {steps.value}\n",
    "              Seed: {actual_seed}\n",
    "              \"\"\"))\n",
    "        with autocast(\"cuda\"):\n",
    "            image = pipe(prompt.value, \n",
    "                generator=torch.Generator(\"cuda\").manual_seed(actual_seed),\n",
    "                num_inference_steps=steps.value, \n",
    "                guidance_scale=cfg.value,\n",
    "                width=width.value,\n",
    "                height=height.value\n",
    "            ).images[0]\n",
    "        del pipe\n",
    "        gc.collect()\n",
    "        with torch.cuda.device(\"cuda\"):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        display(image)\n",
    "            \n",
    "generate_btn.on_click(generate)\n",
    "box = VBox(\n",
    "    children=[\n",
    "        checkpoint, prompt, \n",
    "        HBox([VBox([width, height]), VBox([steps, cfg])]), \n",
    "        seed, \n",
    "        generate_btn, \n",
    "        out]\n",
    ")\n",
    "\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa733fb-d112-4d2f-9771-5fc8ac11e0c8",
   "metadata": {},
   "source": [
    "# 8. Terminate your GPU instance when you are done\n",
    "\n",
    "Don't forget to terminate your cloud GPU instance once you are done evaluating your checkpoints, otherwise you will be still charged. Check the last section of the previous chapter to see how to terminate your instance. \n",
    "\n",
    "Note that once you terminate your instance **Jupyter Lab** will stop working. If you want to use it again you'll have to start a new training session on the same or difference GPU instance, and start all over again. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e677f113ff5b533036843965d6e18980b635d0aedc1c5cebd058006c5afc92a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
