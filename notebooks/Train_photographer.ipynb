{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c831b5b-3025-4177-bef5-25aaec89573a",
   "metadata": {},
   "source": [
    "# **Chapter IV - Running, monitoring and evaluating a training job**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9a96d-668e-40e2-bb63-a24b9213948d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### About this chapter\n",
    "\n",
    "In this chapter, we'll cover the following topics in an interactive way:\n",
    "\n",
    "1. **Fine-tuning a Stable Diffusion base model with a custom dataset**.\n",
    "      \n",
    "2. **Upload the training dataset to the cloud GPU instance**. \n",
    "\n",
    "3. **Start the training job**\n",
    "    \n",
    "3. **Monitor your sample generations in Tensorboard**. Tensorboard is a free tool used to visualise machine learning experiments. No installation is required as it will be run from a standalone webpage.\n",
    "      \n",
    "4. **Upload the fine-tuned models to Hugging Face** so you can re-use them later. Hugging Face is an open-source community for AI experts and enthusiasts. Itâ€™s free to use!\n",
    "       \n",
    "5. **Evaluate the fine-tuned model** to asses its performance.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "This interactive tutorial assumes you have:\n",
    "\n",
    "- Setup the training application on a cloud GPU platform as per the previous chapter.\n",
    "- A basic understanding of how Jupyter Notebook works\n",
    "- A reliable internet connection.\n",
    "- An updated browser such as Chrome, Safari, Firefox, etc. \n",
    "- Time to train (it will take about 20 minutes to train the training dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb35beb-c70a-49f7-a369-ceecdc13fd9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Fine-tuning a Stable Diffusion base model with custom data\n",
    "\n",
    "### Fine-tuning using a photographer's image dataset\n",
    "\n",
    "In this notebook tutorial we will train a Stable Diffusion base model in the style of a photographer that it doesn't knows very well. By feeding the model with a photographer's image dataset, it should be able to generate pictures in the style of the photographer.  \n",
    "\n",
    "### Bella Kotak\n",
    "\n",
    "In this notebook tutorial, we will fine-tune a Stable Diffusion base model using Bella Kotak recent artwork. **Bella Kotak** is an award-winning UK-based photographer with a strong, distinctive style.\n",
    "\n",
    "Check her instagram account at [https://www.instagram.com/bellakotak](https://www.instagram.com/bellakotak) ...and be amazed!\n",
    "\n",
    "### Before fine-tuning the base model\n",
    "\n",
    "Hope you checked her portfolio because you need to know how much the base model needs to learn in order to be able to generate decent-looking synthetic images in her artistic style. \n",
    "\n",
    "Since the base model wasn't trained with enough pictures of her artwork, it fails to portray her unique artistic vision. So if we prompt the base Stable Diffusion 1.5 model with **\"a black and white photo of a woman wearing a floral crown and holding a bouquet of flowers in the style of Bella Kotak\"**, the base model will struggle to generate a picture that represents her style, or even follow the prompt. \n",
    "  \n",
    "![Bella before](https://drive.google.com/uc?export=view&id=1iUX_aMLQCulbcLMEMbta9GRsPk4VVG-i)\n",
    "\n",
    "### After fine-tuning the base model\n",
    "\n",
    "Thankfully by fine-tuning the base Stable Diffusion model using captioned images, the ability of the base model to generate better-looking pictures based on her style is greatly improved. And even the prompt is better followed. \n",
    "\n",
    "Image below was generated on a fine-tuned Stable Diffusion 1.5 model. It has the same prompt, seed, resolution, and CFG values as the image above!\n",
    "\n",
    "![Bella after](https://drive.google.com/uc?export=view&id=1GgOyCNIFAkjsvkVcYc7U3SlgppLXMJPX)\n",
    "\n",
    "As you can see, it's not perfect -for one thing, it's not exactly black and white- but yet the differences between the non-fine tuned model and the fine-tuned one are very noticeable. That's the power of training a Stable Diffusion base model with a custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef18e8-b5aa-41fd-8cc8-3ad07d19307f",
   "metadata": {},
   "source": [
    "# 2. Upload the training dataset\n",
    "\n",
    "### Download and extract the dataset \n",
    "\n",
    "We are going to download an already prepared training dataset into our GPU instance.\n",
    "\n",
    "A dataset is said to be prepared when every image has a caption describing it. It may or may not include other configuration settings read by the training application. \n",
    "\n",
    "Our image dataset contains 109 images, 109 text files, and 1 tag configuration file (`global.yaml`) that adds a suffix tag to each text file (in this case appends the phrase `in the style of Bella Kotak` to each caption description for each image). For more information about how to create a dataset please refer to chapter II of the tutorial.\n",
    "\n",
    "This is an example of how images and caption files are formatted in our dataset:\n",
    "\n",
    "* `image-name_001.jpg`\n",
    "* `image-name_001.txt`  <= Same filename as the jpg file\n",
    "\n",
    "The text file `image-name_001.txt` contains the caption describing `image-name_001.jpg`, say, for example: `a photo of a woman wearing a floral crown and holding a bouquet of flowers in the style of Bella Kotak`.\n",
    "\n",
    "Running the cell below will download a public ZIP file from Google Drive, extract it and store it in the **input** subfolder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9f489-5a4d-4816-b66d-a84a2a00993c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Install gdown (to be able to download files from Google Drive)\n",
    "!pip install gdown\n",
    "\n",
    "# Download dataset\n",
    "path_to_dataset = \"input/dataset.zip\"\n",
    "\n",
    "if not os.path.exists(path_to_dataset):\n",
    "    !gdown 1Ifk07HeqxHfCCOCvb5oDF-cdxfkfsuq- -O input/dataset.zip\n",
    "else:\n",
    "    print(f\"Already downloaded `{path_to_dataset}`\")\n",
    "\n",
    "# Unzip dataset into 'input' folder\n",
    "with zipfile.ZipFile(path_to_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall('input/dataset')\n",
    "\n",
    "# Remove zip file\n",
    "os.remove(path_to_dataset)\n",
    "\n",
    "# List input directory\n",
    "%ls input/\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fcd56-0418-4be1-a5c3-38aa679b1aaf",
   "metadata": {},
   "source": [
    "# 3. Start the training job\n",
    "\n",
    "Once our training images and their captions are inside the **input folder** we are ready to train the model. \n",
    "\n",
    "\n",
    "### Training configuration\n",
    "We will override these default training configuration settings:\n",
    "\n",
    "* **project name**: \"sd1_kotak\" <= Name of the project. It is convenient to name it in a way that identifies it from other training sessions.\n",
    "* **data_root**: \"input\" <= Folder location of the training images\n",
    "* **max epochs**: 60 <= An epoch refers to the one entire passing of training images through the trainer. We are doing 60 entire passes.  \n",
    "* **batch size**: 6 <= Determines the amount of images that are going to be trained every epoch\n",
    "* **sample steps**: 80 <= Determines how frequently samples are generated. In this case we will save every 20 epoch steps.   \n",
    "* **save every n epochs**: 20 <= Checkpoints will be saved every 20 epochs (since we are doing 60 epochs, we will end with 3 checkpoints) \n",
    "* **save ckpt dir**: \"ouput\" <= Folder location of the saved checkpoints\n",
    "* **zero_frequency_noise_ratio**: 0.04 <= This will make dark scenes more realistic  \n",
    "* **optimizer_config**: optimizer-photo.json <= We add an optimiser config file to get better results\n",
    "* **cond_dropout**: 0.0 <= This will prevent the trainer learning images without captions\n",
    "\n",
    "\n",
    "The are more configuration settings but we won't show them here. For a detailed explanation about the trainer configuration visit EveryDream 2's [official documentation](https://github.com/victorchall/EveryDream2trainer/blob/main/doc/TRAINING.md). \n",
    "\n",
    "### Donwload the optimizer configuration file\n",
    "\n",
    "Run the following cell to get the optimiser configuration settings to improve the training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655d4fc-7975-4ca3-bb58-767de9177a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/learn2train/l2t-sd/blob/main/notebooks/optimizer-photo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0a39e-8204-4017-98ef-4ae2451511be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running the training session\n",
    "\n",
    "To start the training run the cell below. The cell will start printing its log. Keep scrolling down to monitor the current status of the training session. \n",
    "\n",
    "**IMPORTANT: If you see messages with a red backround, IGNORE THEM as they are only warning messages** \n",
    "\n",
    "The training takes about 20 minutes on a RTX 3090 with 24GB of VRAM. \n",
    "\n",
    "While you wait for the `Training completed` message, watch the samples being generated (see cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73fb86-ebef-41e2-9382-4aa11be84be6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the training\n",
    "\n",
    "%run train.py --resume_ckpt \"learn2train/stable-diffusion-v1-5\" \\\n",
    "--project_name \"sd1_kotak\" \\\n",
    "--data_root \"input\" \\\n",
    "--max_epochs 60 \\\n",
    "--sample_steps 80 \\\n",
    "--batch_size 6 \\\n",
    "--save_every_n_epochs 20 \\\n",
    "--zero_frequency_noise_ratio 0.04 \\\n",
    "--cond_dropout 0.0 \\\n",
    "--optimizer_config optimizer-photo.json \\\n",
    "--save_ckpt_dir \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0be597-abb7-4f52-b29e-11f18c387edb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Watch your samples in Tensorboard while training is running\n",
    "\n",
    "### Tensorboard dashboard\n",
    "\n",
    "Go to the Tensorboard dashboard in the other browser tab you opened while setting up the trainer as seen in the previous chapter.  \n",
    "\n",
    "When we set up the trainer application there was no data shown. But now that the training is running you will see line graphs and samples being generated. \n",
    "\n",
    "Click on `IMAGES` on the top menu and you will see the latest samples generated. To see them in real-time click on the settings icon -on the top right menu- to reload the screen every 30 seconds. \n",
    "\n",
    "Checking your samples should give you an idea how good/bad your model learning progress is going.\n",
    "\n",
    "Samples are created in a single rectangular image divided in three samples. That is because each sample one is generated using different CFG values (1, 4 and 7).\n",
    "\n",
    "![Tensorboard](https://drive.google.com/uc?export=view&id=1VKisojexs2d5xn5h9IuovkcalzBm1ob1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0982c-6029-4fc1-b161-f71c18680261",
   "metadata": {},
   "source": [
    "# 5. Upload your checkpoints to Hugging Face\n",
    "\n",
    "\n",
    "Once the training is done you should see the following messages:\n",
    "\n",
    "![Training is finished](https://drive.google.com/uc?export=view&id=1WXwNcHaKStpuusvReueriEJXsl3rLWRM)\n",
    "\n",
    "\n",
    "That was it! The base model has been updated in several checkpoints. Before terminating the GPU instance, upload them and save them to your Hugging Face repository.\n",
    "\n",
    "\n",
    "### Log-in into your account \n",
    "\n",
    "Run the cell below and paste your **Hugging Face write token** you got from chapter II into the prompt to log into your account to be able to upload data into your repo (NOTE: There's no need to go to the Hugging Face website: you will be loging in from the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b4a52-97ab-4d70-8db7-43d1203135fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n",
    "\n",
    "from huggingface_hub import notebook_login, hf_hub_download\n",
    "import os\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24eee3d-f5df-45f3-9acc-ee0206cfe6b1",
   "metadata": {},
   "source": [
    "### Upload checkpoints to your model repository\n",
    "\n",
    "Make sure you are **logged in** to Hugging Face running the above login cell first.\n",
    "\n",
    "Use the cell below to upload one or more checkpoints to your personal Hugging Face repository. You should already be authorized to Huggingface by token if you used the download/token cells above.\n",
    "\n",
    "When you run the cell below, a box will show up and you need to  **CLICK** to select which `.ckpt` files are marked for upload. This allows you to select which ones to upload.  If you don't click of the ckpts, nothing will happen.\n",
    "\n",
    "You will also be required to fill-in your username and your repository name:\n",
    "* Hugging Face username: Look for your username in [HuggingFace account page](https://huggingface.co/settings/account).\n",
    "* Hugging Face repository name: **photographer**\n",
    "\n",
    "**WARNING**\n",
    "\n",
    "**If your Hugging Face account is brand new upload only 3 checkpoint files**. For safety reasons, Hugging Face limits the amount of files a new user can make. If you try to upload more than 3 checkpoint files you'll probably get a warning tell you to wait 24 hours to keep uploading. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df5e1a-3c68-41c0-a4ed-ea0abcd19858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after reading the instructions of the cell above. \n",
    "\n",
    "import glob\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from ipywidgets import *\n",
    "\n",
    "all_ckpts = [f for f in glob.glob(\"output/*.ckpt\")]\n",
    "  \n",
    "ckpt_picker = SelectMultiple(options=all_ckpts, layout=Layout(width=\"600px\")) \n",
    "hfuser = Text(placeholder='Hugging Face username')\n",
    "hfrepo = Text(placeholder='Hugging Face repository name')\n",
    "\n",
    "api = HfApi()\n",
    "upload_btn = Button(description='Upload')\n",
    "out = Output()\n",
    "\n",
    "def upload_ckpts(_):\n",
    "    repo_id=f\"{hfuser.value or hfuser.placeholder}/{hfrepo.value or hfrepo.placeholder}\"\n",
    "    with out:\n",
    "        if ckpt_picker is None or len(ckpt_picker.value) < 1:\n",
    "            print(\"Nothing selected for upload, make sure to click one of the ckpt files in the list, or, you have no ckpt files in the current directory.\")\n",
    "        for ckpt in ckpt_picker.value:\n",
    "            print(f\"Uploading to HF: huggingface.co/{repo_id}/{ckpt}\")\n",
    "            response = api.upload_file(\n",
    "                path_or_fileobj=ckpt,\n",
    "                path_in_repo=ckpt,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=None,\n",
    "                create_pr=1,\n",
    "            )\n",
    "            display(response)\n",
    "        print(\"DONE\")\n",
    "\n",
    "upload_btn.on_click(upload_ckpts)\n",
    "box = VBox([ckpt_picker, HBox([hfuser, hfrepo]), upload_btn, out])\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4a28a-2488-4216-b431-fcc3e9d1de9e",
   "metadata": {},
   "source": [
    "### Save the uploads to your model repository\n",
    "\n",
    "To actually save the uploaded checkpoints into your repository, go back to your Hugging Face model repository and click the **Community** tab. You'll see a list of all the uploaded checkpoints. Go one by one and click **Merge** to save them to your model repository:\n",
    "\n",
    "![Merge](https://drive.google.com/uc?export=view&id=1zyOcOq9uABW1dO69pNYenvsag1C7asyc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a00d16-9b84-492f-8e6a-defe71e82b43",
   "metadata": {},
   "source": [
    "# 6. Evaluate the fine-tuned model\n",
    "\n",
    "\n",
    "### Test inference on your checkpoints\n",
    "\n",
    "To recap: Training is over and you are left with model checkpoints. These checkpoints are updated fine-tuned models saved at different times during the training session. \n",
    "\n",
    "The main idea here is to evaluate each of your checkpoints to find the ones that generate the output you like the most.  \n",
    "\n",
    "Run the following cell to display a mini text-to-image generator. You can choose any checkpoint -or all of them- and set inference parameters such as prompt, steps, CFG, resolution and seed.\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1a8cd-6a04-44e5-a770-c23ee247ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, DDIMScheduler, DDPMScheduler, PNDMScheduler, EulerAncestralDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "\n",
    "checkpoints_ts = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if os.path.basename(file) == \"model_index.json\":\n",
    "                ts = os.path.getmtime(os.path.join(root,file))\n",
    "                ckpt = root\n",
    "                checkpoints_ts.append((ts, root))\n",
    "\n",
    "checkpoints = [ckpt for (_, ckpt) in sorted(checkpoints_ts, reverse=True)]\n",
    "full_width = Layout(width='600px')\n",
    "half_width = Layout(width='300px')\n",
    "\n",
    "checkpoint = Dropdown(options=checkpoints, description='Checkpoint:', layout=full_width)\n",
    "prompt = Textarea(value='a photo of ', description='Prompt:', layout=full_width)\n",
    "height = IntSlider(value=512, min=256, max=768, step=32, description='Height:', layout=half_width)\n",
    "width = IntSlider(value=512, min=256, max=768, step=32, description='Width:', layout=half_width)\n",
    "cfg = FloatSlider(value=7.0, min=0.0, max=14.0, step=0.2, description='CFG Scale:', layout=half_width)\n",
    "steps = IntSlider(value=30, min=10, max=100, description='Steps:', layout=half_width)\n",
    "seed = IntText(value=-1, description='Seed:', layout=half_width)\n",
    "generate_btn = Button(description='Generate', layout=full_width)\n",
    "out = Output()\n",
    "\n",
    "def generate(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        display(f\"Loading model {checkpoint.value}\")\n",
    "        actual_seed = seed.value if seed.value != -1 else random.randint(0, 2**30)\n",
    "\n",
    "        text_encoder = CLIPTextModel.from_pretrained(checkpoint.value, subfolder=\"text_encoder\")\n",
    "        vae = AutoencoderKL.from_pretrained(checkpoint.value, subfolder=\"vae\")\n",
    "        unet = UNet2DConditionModel.from_pretrained(checkpoint.value, subfolder=\"unet\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(checkpoint.value, subfolder=\"tokenizer\", use_fast=False)\n",
    "        scheduler = DDIMScheduler.from_pretrained(checkpoint.value, subfolder=\"scheduler\")\n",
    "        text_encoder.eval()\n",
    "        vae.eval()\n",
    "        unet.eval()\n",
    "\n",
    "        text_encoder.to(\"cuda\")\n",
    "        vae.to(\"cuda\")\n",
    "        unet.to(\"cuda\")\n",
    "\n",
    "        pipe = StableDiffusionPipeline(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=None, # save vram\n",
    "            requires_safety_checker=None, # avoid nag\n",
    "            feature_extractor=None, # must be none of no safety checker\n",
    "        )\n",
    "\n",
    "        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "        \n",
    "        print(inspect.cleandoc(f\"\"\"\n",
    "              Prompt: {prompt.value}\n",
    "              Resolution: {width.value}x{height.value}\n",
    "              CFG: {cfg.value}\n",
    "              Steps: {steps.value}\n",
    "              Seed: {actual_seed}\n",
    "              \"\"\"))\n",
    "        with autocast(\"cuda\"):\n",
    "            image = pipe(prompt.value, \n",
    "                generator=torch.Generator(\"cuda\").manual_seed(actual_seed),\n",
    "                num_inference_steps=steps.value, \n",
    "                guidance_scale=cfg.value,\n",
    "                width=width.value,\n",
    "                height=height.value\n",
    "            ).images[0]\n",
    "        del pipe\n",
    "        gc.collect()\n",
    "        with torch.cuda.device(\"cuda\"):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        display(image)\n",
    "            \n",
    "generate_btn.on_click(generate)\n",
    "box = VBox(\n",
    "    children=[\n",
    "        checkpoint, prompt, \n",
    "        HBox([VBox([width, height]), VBox([steps, cfg])]), \n",
    "        seed, \n",
    "        generate_btn, \n",
    "        out]\n",
    ")\n",
    "\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa733fb-d112-4d2f-9771-5fc8ac11e0c8",
   "metadata": {},
   "source": [
    "# 6. Terminate your GPU instance when you are done\n",
    "\n",
    "Don't forget to terminate your cloud GPU instance once you are done evaluating your checkpoints, otherwise you will be still charged. Check the last section of the previous chapter to see how to terminate your instance. \n",
    "\n",
    "Note that once you terminate your instance, both **Tensorboard** and **Jupyter Lab** will stop working, and if you want to use them again you'll have to start a new training session on a new GPU instance, and setup Tensorboard and Jupyter Lab to start all over again. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e677f113ff5b533036843965d6e18980b635d0aedc1c5cebd058006c5afc92a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
